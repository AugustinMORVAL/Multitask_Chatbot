models:
  - mixtral-8x7b-32768
  - llama3-70b-8192
  - gemma2-9b-it

temperature_slider:
  min_value: 0.0
  max_value: 1.0
  value: 0.2
  step: 0.1
  help: "Controls the randomness of the model's responses."

system_prompt:
  value: "You are a helpful assistant."
  height: 100
  help: "Sets the context and behavior for the conversation."

cot_reflection:
  value: |
    You are an advanced AI assistant that uses a sophisticated Chain of Thought (CoT) approach with multi-stage reflection to answer queries. Follow these steps:

    1. Initial Analysis: Analyze the query and break it down into key components.
    2. Research: If needed, outline any research or information gathering steps.
    3. Reasoning: Think through the problem step by step.
    4. First Reflection: Reflect on your reasoning to check for errors, biases, or improvements.
    5. Refinement: Make adjustments based on your reflection.
    6. Second Reflection: Perform a final check on your refined reasoning.
    7. Conclusion: Synthesize your thoughts into a concise, clear answer.
    8. Output: Provide your final, polished answer. Don't hesitate to get into details the answer must provide all the necessary information to respond the question. 
    9. Resources: If needed, provide the resources used to answer the question.

    Use the following format for your response:
    <thinking>
    [Your step-by-step reasoning goes here.]
    <reflection>
    [Your reflection on your reasoning, checking for errors or improvements]
    </reflection>
    [Any adjustments to your thinking based on your reflection]
    </thinking>
    <output>
    [Your final answer to the query with all resources used to answer the question.]
    </output>
  height: 100
  help: "Sets a prompt for advanced problem-solving with multi-stage reflection, improving accuracy, transparency, and depth of reasoning."
  tags:
    thinking: ğŸ§ 
    reflection: ğŸ’¡
    output: ğŸ“¤

max_tokens_slider:
  min_value: 100
  max_value: 4096
  value: 500
  step: 100
  help: "Controls the maximum number of tokens in the model's responses."

max_tokens_slider_cot_reflection:
  min_value: 100
  max_value: 4096
  value: 4096
  step: 100
  help: "Controls the maximum number of tokens in the model's responses."

additional_parameters:
  top_p:
    label: "Top P"
    slider:
      min_value: 0.0
      max_value: 1.0
      value: 1.0
      step: 0.1
      help: "Set the probability threshold for generating a response."
  frequency_penalty:
    label: "Frequency Penalty"
    slider:
      min_value: 0.0
      max_value: 2.0
      value: 0.0
      step: 0.1
      help: "Penalize the model for repeating the same words or phrases."
  presence_penalty:
    label: "Presence Penalty"
    slider:
      min_value: 0.0
      max_value: 2.0
      value: 0.0
      step: 0.1
      help: "Penalize the model for using words or phrases already used in the conversation."
  stop:
    label: "Stop Sequences"
    input:
      value: ""
      help: "Specify sequences that the model should stop generating a response when encountered."

file_icons:
  pdf: ğŸ“„
  docx: ğŸ“
  audio*: ğŸµ
  audio/wav: ğŸ”Š
  audio/mpeg: ğŸ§
  audio/ogg: ğŸ¼
  audio/flac: ğŸ”‰
  audio/aac: ğŸ¶
  audio/m4a: ğŸ™ï¸