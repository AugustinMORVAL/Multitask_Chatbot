models:
  - mixtral-8x7b-32768
  - llama3-70b-8192
  - gemma2-9b-it

temperature_slider:
  min_value: 0.0
  max_value: 1.0
  value: 0.2
  step: 0.1
  help: "Controls the randomness of the model's responses."

system_prompt:
  value: "You are a helpful assistant."
  height: 100
  help: "Sets the context and behavior for the conversation."

cot_reflection:
  value: |
    You are an advanced AI assistant that uses a sophisticated Chain of Thought (CoT) approach with multi-stage reflection to answer queries. Follow these steps:

    1. Initial Analysis: Analyze the query and break it down into key components.
    2. Research: If needed, outline any research or information gathering steps.
    3. Reasoning: Think through the problem step by step.
    4. First Reflection: Reflect on your reasoning to check for errors, biases, or improvements.
    5. Refinement: Make adjustments based on your reflection.
    6. Second Reflection: Perform a final check on your refined reasoning.
    7. Conclusion: Synthesize your thoughts into a concise, clear answer.
    8. Output: Provide your final, polished answer. Don't hesitate to get into details the answer must provide all the necessary information to respond the question. 
    9. Resources: If needed, provide the resources used to answer the question.

    Use the following format for your response:
    <thinking>
    [Your step-by-step reasoning goes here.]
    <reflection>
    [Your reflection on your reasoning, checking for errors or improvements]
    </reflection>
    [Any adjustments to your thinking based on your reflection]
    </thinking>
    <output>
    [Your final answer to the query with all resources used to answer the question.]
    </output>
  height: 100
  help: "Sets a prompt for advanced problem-solving with multi-stage reflection, improving accuracy, transparency, and depth of reasoning."
  tags:
    thinking: 🧠
    reflection: 💡
    output: 📤

max_tokens_slider:
  min_value: 100
  max_value: 4096
  value: 500
  step: 100
  help: "Controls the maximum number of tokens in the model's responses."

max_tokens_slider_cot_reflection:
  min_value: 100
  max_value: 4096
  value: 4096
  step: 100
  help: "Controls the maximum number of tokens in the model's responses."

additional_parameters:
  top_p:
    label: "Top P"
    slider:
      min_value: 0.0
      max_value: 1.0
      value: 1.0
      step: 0.1
      help: "Set the probability threshold for generating a response."
  frequency_penalty:
    label: "Frequency Penalty"
    slider:
      min_value: 0.0
      max_value: 2.0
      value: 0.0
      step: 0.1
      help: "Penalize the model for repeating the same words or phrases."
  presence_penalty:
    label: "Presence Penalty"
    slider:
      min_value: 0.0
      max_value: 2.0
      value: 0.0
      step: 0.1
      help: "Penalize the model for using words or phrases already used in the conversation."
  stop:
    label: "Stop Sequences"
    input:
      value: ""
      help: "Specify sequences that the model should stop generating a response when encountered."

file_icons:
  pdf: 📄
  docx: 📝
  audio*: 🎵
  audio/wav: 🔊
  audio/mpeg: 🎧
  audio/ogg: 🎼
  audio/flac: 🔉
  audio/aac: 🎶
  audio/m4a: 🎙️