models:
  llama-3.1-70b-versatile:
    context_window: 32768
    description: |
      High-performance large language model optimized for complex tasks and reasoning.
      Best suited for tasks requiring deep understanding, analysis, and sophisticated outputs.
      Excellent choice when quality is prioritized over speed.
    use_case:
      - "Complex analysis and research"
      - "In-depth explanations"
      - "Advanced problem-solving"
      - "Academic writing"
      - "Complex code review"
      - "Technical documentation"
      - "Research synthesis"

  llama-3.1-8b-instant:
    context_window: 131072
    description: |
      Lightweight, efficient model optimized for speed and basic tasks.
      Excellent for quick queries and simple information retrieval.
      Provides fast, concise responses with minimal latency.
    use_case:
      - "Quick fact checking"
      - "Simple calculations"
      - "Basic Q&A"
      - "Data validation"
      - "Short summaries"
      - "Simple classifications"

  llama-3.2-90b-vision-preview:
    context_window: 8192
    description: |
      Multimodal model specialized in vision-language tasks.
      Excellent for image understanding and visual reasoning.
      Can process and analyze images alongside text.
    use_case:
      - "Image analysis"
      - "Visual question answering"
      - "Image-based troubleshooting"
      - "Chart/diagram interpretation"
      - "Visual content description"

models_selection_criteria:
  priority_factors:
    - task_complexity:
        simple: "llama-3.1-8b-instant"
        moderate: "llama-3.1-70b-versatile"
        complex: "llama-3.1-70b-versatile"
    
    - response_speed:
        critical: "llama-3.1-8b-instant"
        moderate: "llama-3.1-70b-versatile"
        flexible: "llama-3.1-70b-versatile"
    
    - context_length:
        short: "any"
        medium: "llama-3.1-70b-versatile"
        long: "llama-3.1-8b-instant"
    
    - task_type:
        coding: "llama-3.1-70b-versatile"
        visual: "llama-3.2-90b-vision-preview"
        analysis: "llama-3.1-70b-versatile"
        quick_lookup: "llama-3.1-8b-instant"

system_prompt:
  value: |
    You are a helpful AI assistant. Your responses should be:
    1. Professional and courteous
    2. Accurate and well-researched
    3. Clear and concise
    4. Properly formatted when dealing with code or technical content

    If you're unsure about something, you should acknowledge that uncertainty.
    
    Remember to:
    - Cite sources when referencing external information
    - Show your work when doing calculations
    - Ask for clarification when needed
    - Use appropriate formatting for code and technical content
  height: 100
  help: "Sets the context and behavior for the conversation."

file_icons:
  pdf: üìÑ
  docx: üìù
  audio*: üéµ
  audio/wav: üîä
  audio/mpeg: üéß
  audio/ogg: üéº
  audio/flac: üîâ
  audio/aac: üé∂
  audio/m4a: üéôÔ∏è

temperature_slider:
  min_value: 0.0
  max_value: 1.0
  value: 0.2
  step: 0.1
  help: "Controls the randomness of the model's responses."
cot_reflection:
  value: |
    You are an advanced AI assistant that uses a sophisticated Chain of Thought (CoT) approach with multi-stage reflection to answer queries. Follow these steps:

    1. Initial Analysis: Analyze the query and break it down into key components.
    2. Research: If needed, outline any research or information gathering steps.
    3. Reasoning: Think through the problem step by step.
    4. First Reflection: Reflect on your reasoning to check for errors, biases, or improvements.
    5. Refinement: Make adjustments based on your reflection.
    6. Second Reflection: Perform a final check on your refined reasoning.
    7. Conclusion: Synthesize your thoughts into a concise, clear answer.
    8. Output: Provide your final, polished answer. Don't hesitate to get into details the answer must provide all the necessary information to respond the question. 
    9. Resources: If needed, provide the resources used to answer the question.

    Use the following format for your response:
    <thinking>
    [Your step-by-step reasoning goes here.]
    <reflection>
    [Your reflection on your reasoning, checking for errors or improvements]
    </reflection>
    [Any adjustments to your thinking based on your reflection]
    </thinking>
    <output>
    [Your final answer to the query with all resources used to answer the question.]
    </output>
  height: 100
  help: "Sets a prompt for advanced problem-solving with multi-stage reflection, improving accuracy, transparency, and depth of reasoning."
  tags:
    thinking: üß†
    reflection: üí°
    output: üì§

max_tokens_slider:
  min_value: 100
  max_value: 4096
  value: 500
  step: 100
  help: "Controls the maximum number of tokens in the model's responses."

max_tokens_slider_cot_reflection:
  min_value: 100
  max_value: 4096
  value: 4096
  step: 100
  help: "Controls the maximum number of tokens in the model's responses."

additional_parameters:
  top_p:
    label: "Top P"
    slider:
      min_value: 0.0
      max_value: 1.0
      value: 1.0
      step: 0.1
      help: "Set the probability threshold for generating a response."
  frequency_penalty:
    label: "Frequency Penalty"
    slider:
      min_value: 0.0
      max_value: 2.0
      value: 0.0
      step: 0.1
      help: "Penalize the model for repeating the same words or phrases."
  presence_penalty:
    label: "Presence Penalty"
    slider:
      min_value: 0.0
      max_value: 2.0
      value: 0.0
      step: 0.1
      help: "Penalize the model for using words or phrases already used in the conversation."
  stop:
    label: "Stop Sequences"
    input:
      value: ""
      help: "Specify sequences that the model should stop generating a response when encountered."