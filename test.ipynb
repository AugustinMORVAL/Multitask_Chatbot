{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchvision is not available - cannot save figures\n",
      "2024-07-23 08:28:33.763 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Users\\augustin.morval\\OneDrive - Wavestone\\Bureau\\Dev Workspace\\Tools\\.venv\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2024-07-23 08:28:33.763 Session state does not function when running a script without `streamlit run`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Good morning/afternoon. I'm [Your Name], a software engineer with [Number] years of experience in developing and maintaining high-quality software solutions.  I specialize in [Mention 2-3 Core Skills e.g., web development, data structures, cloud computing] and have a proven track record of successfully delivering projects on time and within budget.  \n",
      "\n",
      "In my previous role at [Previous Company], I was responsible for [Briefly describe a key achievement or responsibility]. I'm proficient in [List 3-4 relevant programming languages and technologies] and am passionate about [Mention a specific area of software development that interests you]. I'm eager to learn new technologies and contribute to a team-oriented environment.\" \n",
      "\n",
      "\n",
      "Please remember to:\n",
      "\n",
      "* **Tailor this to the specific job description.** Highlight skills and experiences directly relevant to the role.\n",
      "* **Quantify your achievements whenever possible.**  Instead of \"Improved efficiency,\" say \"Improved efficiency by 15%.\"\n",
      "* **Practice your delivery.**  Speak clearly and confidently. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from app.app import Groq\n",
    "\n",
    "load_dotenv()\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "client = Groq(api_key=GROQ_API_KEY)\n",
    "prompt = \"Present yourself as a professional and experienced software engineer. You are speaking to a potential employer. Introduce yourself and explain your skills and experience. Be concise and professional.\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gemma2-9b-it\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": prompt},\n",
    "    ],\n",
    "    temperature=0.8,\n",
    "    max_tokens=1000\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemma2-9b-it\n",
      "gemma-7b-it\n",
      "llama3-70b-8192\n",
      "llama3-8b-8192\n",
      "llama3-groq-70b-8192-tool-use-preview\n",
      "llama3-groq-8b-8192-tool-use-preview\n",
      "mixtral-8x7b-32768\n",
      "whisper-large-v3\n"
     ]
    }
   ],
   "source": [
    "models = client.models.list()\n",
    "for model in models.data:\n",
    "    print(model.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12256\\1545597013.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAutoFeatureExtractor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mset_seed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mparler_tts\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mParlerTTSForConditionalGeneration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cpu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoFeatureExtractor, set_seed\n",
    "from parler_tts import ParlerTTSForConditionalGeneration\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "repo_id = \"parler-tts/parler-tts-mini-v1\"\n",
    "model = ParlerTTSForConditionalGeneration.from_pretrained(repo_id).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(repo_id)\n",
    "\n",
    "SAMPLE_RATE = feature_extractor.sampling_rate\n",
    "SEED = 42\n",
    "\n",
    "def gen_tts(text, description):\n",
    "    inputs = tokenizer(description.strip(), return_tensors=\"pt\").to(device)\n",
    "    prompt = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    set_seed(SEED)\n",
    "    generation = model.generate(\n",
    "        input_ids=inputs.input_ids,\n",
    "        prompt_input_ids=prompt.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        prompt_attention_mask=prompt.attention_mask,\n",
    "        do_sample=True,\n",
    "        temperature=1.0\n",
    "    )\n",
    "    audio_arr = generation.cpu().numpy().squeeze()\n",
    "\n",
    "    return SAMPLE_RATE, audio_arr\n",
    "\n",
    "text = input(\"What do you want to say? \")\n",
    "description = \"The voice should be clear, articulate, and moderately paced, with a slightly higher pitch for engagement. Consistent tone and volume are desired.\"\n",
    "sample_rate, audio_arr = gen_tts(text, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
